{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "import pdb\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, BitsAndBytesConfig\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "from llavaAgent import LLaVaAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = LLaVaAgent('mistral', '7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "im = cv2.imread(\"images_for_vlm/scene1.png\")\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "pt = (\"You are a robot navigating within an 3-D environment as shown. In the image you see, there are \"\n",
    "                  f\"{4} labeled objects. You will be asked to analyze the spatial position of these labeled \"\n",
    "                  f\"objects with relation to each other. The red dots on each object are the object's center of mass, \"\n",
    "                  f\"which you should use when comparing the position of two objects. From your point of view, \"\n",
    "                  f\"answer each question with the\"\n",
    "                  f\"descriptors right/left, above/below, in front/behind. If there is no clear spatial difference \"\n",
    "                  f\"along a given axis, you can answer 'neutral'\")\n",
    "pt += (\"\\n1. Where is the led tv in relation to the couch?\\n2. Where is the couch in relation to the chandelier?\\n3. Where is the chandelier in relation to the kitchen countertop item?\")\n",
    "\n",
    "pt +=  (\"\\nReason through the task  and describe the 3d layout of the image you see. Output your thinking and at the very end output \"\n",
    "                   \"'ANSWER' followed by a json object in the following example format:\"\n",
    "                   \"\\n{1: ['right', 'above', 'neutral'], 2: ['left', 'neutral', 'in front']}\\nThere should be 3 key-value pairs corresponding to the 3 questions\")\n",
    "\n",
    "out, eff = agent.call(im, pt, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.index('{')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "                                                          quantization_config=quantization_config,\n",
    "                                                          device_map=\"auto\",\n",
    "                                                          torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    "                                                          )\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "import cv2\n",
    "im = cv2.imread(\"images_for_vlm/scene1.png\")\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "image = Image.fromarray(im, mode='RGB')\n",
    "pt = (\"You are a robot navigating within an 3-D environment as shown. In the image you see, there are \"\n",
    "                  f\"{4} labeled objects. You will be asked to analyze the spatial position of these labeled \"\n",
    "                  f\"objects with relation to each other. The red dots on each object are the object's center of mass, \"\n",
    "                  f\"which you should use when comparing the position of two objects. From your point of view, \"\n",
    "                  f\"answer each question with the\"\n",
    "                  f\"descriptors right/left, above/below, in front/behind. If there is no clear spatial difference \"\n",
    "                  f\"along a given axis, you can answer 'neutral'\")\n",
    "pt += (\"\\n1. Where is the led tv in relation to the couch?\\n2. Where is the couch in relation to the chandelier?\\n3. Where is the chandelier in relation to the kitchen countertop item?\")\n",
    "\n",
    "pt +=  (\"\\nReason through the task  and describe the 3d layout of the image you see. Output your thinking and at the very end output \"\n",
    "                   \"'ANSWER' followed by a json object in the following example format:\"\n",
    "                   \"\\n{1: ['right', 'above', 'neutral'], 2: ['left', 'neutral', 'in front']}\\nThere should be 3 key-value pairs corresponding to the 3 questions\")\n",
    "\n",
    "\n",
    "prompt = f\"[INST] <image>\\n{pt} [/INST]\"\n",
    "#prompt = f\"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\n{pt} ASSISTANT:\"\n",
    "\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print('starting output')\n",
    "t = time.time()\n",
    "input_tokens = inputs['input_ids'].shape[1]\n",
    "print(f'prompt tokens: {input_tokens}')\n",
    "output = model.generate(**inputs, max_new_tokens=1000)\n",
    "duration = time.time() - t\n",
    "print(f'finished output, took {duration} seconds')\n",
    "output_tokens = output.shape[1]\n",
    "print(f'output tokens: {output_tokens-input_tokens}')\n",
    "print(f'efficiency, {(output_tokens-input_tokens)/duration} tokens per second')\n",
    "print(processor.decode(output[0][input_tokens:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(type(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SBVLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
